
--- Page 1 ---
Adding Structure to Your Text Data
Anatomy of a Text Processing Pipeline
MAR 03, 2025
Share
I thought I w as doing a bu nch of different things over the past couple of years, b ut i
turns out they w ere m ostly variations on a theme: Processing U nstructured Text D
Taking U SAJ obs  posts and extracting names of software tools and programming
languages? Processing Unstructured Text Data.
Thanks for reading The Present of Coding!
Subscribe for free to receive new posts and
support my work.
Creating a w ebs ite that uses ModernBERT to predict if something is edible? Process
Unstructured Text Data.
W riting grading rubrics for synthetic data? Pulling content from PowerPoints to
create one-slide summaries? A ll of these are Processing Unstructured Text Data.
In each case, I have a set of documents and I'm  asking the same questions about eac
of them. F or instance: what software tools and programming languages appear in each jo
posting?, what are the key talking points from these slides?, o r did this set of directions
generated by an LLM  contain a particular step?ABIGAIL HADDAD
2
What Defines This Workflow?7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 1/11
--- Page 2 ---
Th is is distinct from a RAG chatbot, w here users ask an open-ended set of question
In m y w orkflo ws, w e’re looking for the same pieces of information from each of the
documents, and  that m ight change over time, b ut w e don't have to answer just any
question that a user m ight ask— and there often aren't users directly interacting w it
the system, just rev iewing output or taking an action based on the results.
Th at said, p art of the evaluation portion of RAG systems does fit into this pattern.
W hen you're u sing tools like RAGAS  to evaluate your RAG pipeline, y ou're taking
chunks of text that w ere passed to the LLM  via the re trieval step, a s w ell as the LLM
output, and  you're as king questions like "is the output properly grounded in the sou
text?" Th at evaluation process is fundamentally the same w orkflow—p rocessing
unstructured text w ith the same question applied re peatedly.
Th is pattern clearly keeps showing up in m y w ork—b ut it took m e awhile to realize
and to re alize that there w ere things I'd learned from one project that I w as bringin
to other ones.
In this post, I'll bre ak down this w orkflow pattern, d iscuss some tools for each of th
steps, and  share w hat I've learned about structuring these projects effectively.
Thes e projects typically follow a three-p art structure:The Three-Part Pipeline7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 2/11
--- Page 3 ---
Get text: Th is m ight involve getting text directly from users, p ulling it from
PowerPoints or PDFs, o r collecting it from an LLM  that you’re evaluating.
Process t hat text: Th is m ight involve an LLM , bu t it also m ight involve regular
expressions for pattern m atching, B ERT for classification, o r another single-
purpose m odel like language classification or translation.
Do som ething w ith the re su lts: S ometimes this is just m aking heat m aps or oth
result visualizations. S ometimes I'm  displaying results for a user, o r populating
PowerPoint slides or other templates w ith structured data.
Let's explore each stage of this pipeline in m ore detail, looking at the specific tools
and approaches I've found useful.
Get Text7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 3/11
--- Page 4 ---
Sometimes the text m ight be given to you directly, sometimes you m ight have to pu
out of a document. Bu t if a file has text, y ou can get it. T here are packages for nearly
any format.
M icroso ft O ffi ce Files: python-docx for W ord documents, python-pptx for
PowerPoint files, PyPDF2 or pdfplumber for PDFs
Structured D ata: pandas for Excel and C SV files, json for JSON d ata
W eb content: beautifulsoup4 and requests for HTM L and X M L
Specialized Formats: L ibr aries for email archives like libraton, pytesseract for
OCR, and  various other packages for specialized formats you m ight encounter 
your specific domain
There are a lso a couple of use cases w here "getting the text" involves an LLM .
The m ajor one is w hen the text you're processing is synthetic text data: text generat
by  an LLM  that you’re then going to evaluate. I'll talk about that in the next section
because I use the same tools to interact w ith LLM s w hether it's for getting text or
processing text.
The o ther is a special case of "getting text" w here you're extracting images and gett
a m ultimodal m odel—I  use Gemini—t o "convert" them into a usable text descriptio
In the project w here I'm  doing this, I use a specific prompt w here I tell the m odel
what I'm  interested in: an y text that's in the image, p lus any data or anything else th
conveys an overall m eaning. I also say w hat to ignore, like colors and formatting—
unless they're co mmunicating data.
I haven't tested this yet to see how w ell it's w orking, b ut if I continue to develop tha
project, I should add in m ore formal testing of that step, b ecause it's not the same a
just pulling actual text out of documents—i t's a m achine learning m odel. T here are
other text processing tools w hich are also m achine learning m odels: for instance, if
you're O CRing text or transcribing text from audio. D epending on your use case, y o7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 4/11
--- Page 5 ---
may treat them like settled technology, o r you m ay need to test their performance w
your specific use case.
Once you've gotten the text, the next step is processing it to extract m eaningful
information. Th is m ight be actual "extraction", in the sense of finding a substring i
your text. O ther times, this is a classification task: for instance, labeling output from
RAG as "grounded" or "not grounded" in the text chunks that w ere passed to it.
Finally, this m ight be generative: synthesizing text to produce something like a
summary.
Just like w ith getting text, there are specialized tools depending on w hat you're tryi
to accomplish:
Lexical Pattern M atching: Regular expressions, string operations, and  fuzzy
matching tools for extracting structured data or finding specific patterns in ter
of the characters in the text.
Specialized M odels: Th is is a huge bu cket that includes language identification
and translation, Named Entity Recognition (that's finding entity classes like
names or places or people— or software tools and programming language), o r
BERT or rel ated m odels for classification. Y ou m ight use these m odels out of th
box, o r you m ight fine-tune them on your specific data.
LLM s: It's rare that I have a w orkflo w for processing unstructured text data tha
doesn't involve LLM s. The le xical pattern m atching and specialized m odels I u
typically are add-ons to the m ain LLM  w orkflow, w hich is sometimes
classification and sometimes generative.Process Text7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 5/11
--- Page 6 ---
I w rote m ore here about the factors that affect w hether I choose an LLM  for any
particular text processing problem. B ut w hen I am, the issues that affect both w hich
models I use and w hich tools I access them w ith are:
The need to ru n m odels l ocally: I can't always send m y data out to a proprietar
LLM .
Swapping out m odels/p roviders: I w ant to switch m odels and providers easily i
my code.
Getting 'structured output': I frequently w ant the LLM  to return a JSON objec
with the output structured in a particular w ay—f or instance, a  list of
programming languages, o r a PAS S or FAI L in one field and an explanation of t
reasoning in another field.
There are m any different tools that provide these features; I'm  not doing a
comprehensive ru ndown here. But the two I've used the m ost are Ollama, for ru nnin
models locally, and  LiteLLM , w hich can w ork either w ith non-local m odels or via
Ollama for local ones. They  both m ake it easy to swap out m odel names, w hich is
especially good for testing or just for developing using a cheaper or faster m odel. A
7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 6/11
--- Page 7 ---
they both support structured outputs. (Although not every m odel allows this, n o m a
what tools you’re using to access it.)
After processing the text data, we need to do something m eaningful w ith that
information. W hat w e do w ith the re sults is just as varied as the previous steps.
Showing it directly to a user: I don't have this w orkflo w very m uch, b ut you m i
have a tool that's designed to directly take input from users and return the m od
answer directly, w ith no other context— or m aybe w ith a probability score of so
sort.
Summarizing it: S ometimes you don't need to show anyone the output directly
bu t you need to summarize it. F or instance, if you're assessing different LLM s f
their ability to rem ain grounded on different types of questions, y ou m ight w an
heat m ap showing each LLM 's performance on some set of test questions.
Populating a template: I'm  starting to get into this w ith a project w here I
populate a PowerPoint template w ith various fields that an LLM  has produced 
the previous step. P owerPoint doesn't m ake this easy, b ut there are other file
formats that are m ore amenable to being populated this w ay.
For projects that aren't one-off but ongoing, y ou'll likely w ant a m onitoring process
that fla gs w hen something bre aks and gives you a path to fix it—f or instance, for LL
classifiers, by  switching m odels or tweaking prompts.
I haven't bu ilt this out yet, bu t I see two m ain approaches. O ne is leveraging natura
labeling processes, w here your models predict something that can be verified later
through re al-w orld outcomes or user actions, a llowing your m onitoring system to
track performance using standard m etrics.Do Something With The Results
The Fourth Step: Monitoring7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 7/11
--- Page 8 ---
But w ithout that—and  I suspect in m ost cases of this w orkflow, y ou w on’t have that
you'll need periodic sampling and review of outputs. M aybe that can be done w ith a
bigger, m ore capable LLM —bu t then, y ou have to m onitor that one as w ell! M ore
likely, y ou need human rev iew.
The k ey here is being strategic about w hich outputs to select for review, m aximizin
the value of rev iewer time. Th is strategy w ill be specific to your use case—f or instan
prioritizing examples w here confidence scores are borderline, in cases w here you h
confidence scores, o r sampling from new types of documents your system hasn't see
before. E ither w ay, this feedback loop is important for keeping text processing
pipelines rel iable over time.
As these projects have evolved, I've developed certain patterns and practices to m ak
the w ork m ore m anageable. I'm  not a software developer and I'm  not w riting softwa
—bu t I still need code I can debu g and build on.
Here are s ome practices I've adopted to varying degrees w ith that in m ind:
M odular C omponents: I structure m y code so text extraction, p rocessing, a nd
output generation are separate components that can be ru n independently. T hi
particularly important because the text processing step typically takes the long
and sometimes costs m oney, if I'm  hitting an external LLM . I don't w ant to re-r
it every time I'm  tweaking m y heat m ap formatting.
Ease of Testing: I need to be able to easily vary LLM  m odels, a nd sometimes al
parameters like token count, temperature, o r prompt, to observe how they affec
results. The s implest approach is using lists of m odels and temperatures in the
code itself. F or m ore complex scenarios, I use text or JSON files to populate
different fields of a class w hich handles the details of the LLM  call.Design Patterns and Ongoing Challenges7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 8/11
--- Page 9 ---
Logging: I've started structuring m y code so that, e ach time anything runs, the
results output into a new folder w ith a name that reflects the date and time it
started ru nning. Th is folder contains not just each of the outputs but also a log 
with everything that happened. W ith LLM s, the benefit of generating these kin
of log files is higher, b ecause if something breaks, y ou can dump the log file int
an LLM  and see if it immediately spots w hat broke.
This is one way to structure a very simple repo
Th at said, I'm  still ru nning into challenges w here I feel like I need m ore backgroun
software development.
The bi ggest one is that I can code quickly w ith LLM s, so I try stuff out that I w ould
otherwise, like spinning up a new classifier or specialized m odel and getting it to w
on a sample string. Bu t that m eans there's a lot of code, a nd it's not necessarily
structured in a w ay that's w ell-architected for m y actual w orkflow.
For instance, if I have three different LLM  classifiers, p lus several smaller or
specialized m odels—and  I w ant to be able to easily change w hich of those I'm  runn
as part of m y text processing pipeline, a s w ell as swap out prompts for m y classifier
order to test them—m y code needs to be really m odular and w ell-structured not jus
across each step of m y w orkflo w but w ithin the text processing step itself.
7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 9/11
--- Page 10 ---
No w that I'm  looking for it, I'm  seeing this "process a stack of documents to extrac
the same set of information" pattern everywhere, in other peoples' u se cases as w ell
mine: fi nding fraudulent bi lling in m edical re cords, d etermining w hat project
someone is w orking on for bi lling purposes based on their computer activity, w ritin
new proposals based on previous w ork and new client requirements.
W hat m akes this pattern distinct is that you're  not answering ad-h oc questions –
you're s ystematically extracting the same information or m aking the same judgmen
across a set of documents. W as there fraud? W hich project should this bill to? W ha
expertise do w e have that's rel evant to a new contract?
And  even though each specific implementation m ight feel re ally different, h aving th
mental m odel is helpful as far as not just re -using code, b ut re-using design pattern
If you're w orking w ith unstructured text data, I hope identifying this pattern helps y
too.
Got a project involving unstructured text or image data you'd like help with? I'm available f
part-time consulting and coding. You can reach me at abigail.haddad at gmail.
Thanks for reading The Present of Coding!
Subscribe for free to receive new posts and
support my work.
2 Likes
Discussion about this postThis Pattern Is Everywhere
7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 10/11
--- Page 11 ---
Write a comment...
© 2025Abigail Haddad ∙ Privacy ∙ Terms ∙ Collection notice
Substackis the home for great cultureCommentsRestacks7/14/25, 1:01 PM (1) Adding Structure to Your Text Data - by Abigail Haddad
https://presentofcoding.substack.com/p/adding-structure-to-your-text-data 11/11