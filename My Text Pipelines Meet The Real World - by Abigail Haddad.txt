
--- Page 1 ---
My Text Pipelines Meet The Real World
Analyzing 35k comments on Schedule F
JUN 23, 2025
Share
A f ew m onths ago, I re alized that m ost of m y projects—f or w ork and not-w ork— ha
similar structure:
Get text: Th is m ight involve getting text directly from users, p ulling it from
PowerPoints or PDFs, o r collecting it from an LLM  that you're evaluating.
Process that text: Th is m ight involve an LLM , bu t it also m ight involve regular
expressions for pattern m atching, B ERT for classification, o r another single-
purpose m odel like language classification or translation.
Do som ething with the results: Sometimes this is just m aking heat m aps or oth
result visualizations. S ometimes I'm  displaying results for a user, o r populating
PowerPoint slides or other templates w ith structured data.
Via m y various projects, I’d w ritten code and picked up some design patterns I liked
So I set out to bu ild something that w ould show this approach in action—w hile also
making something useful.
Thanks for reading The Present of Coding!
Subscribe for free to receive new posts and
support my work.
An alyzing public comments on the proposed reg ulation to rem ove civil service
employment protections from m any federal government employees seemed like a grABIGAIL HADDAD
17/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 1/12
--- Page 2 ---
use case. And  how different could it be from w hat I'd done before?
This is the proposed rule that I analyzed the comments on
The b asic structure w as the same:
Get text: U se the AP I to download comments from re gulations.gov
Process text: U se LLM s to categorize stance and themes
Do som ething: Bu ild a searchable w eb i nterface to explore results
M y goal w as to use existing code and build new m odular elements that I could
continue to re -use on future projects.
The p roject ended w ell. Bu t it didn’t go the w ay I’d imagined.
Instead, I learned that I had a few Lego blocks for different capabilities, b ut I neede
W AY M ORE. E ach new req uirement—s cale, the ability to re sume processing, a  real
frontend— demanded specialized pieces I hadn't built before, some of w hich w ere
unique to this project and I probably w on't need again.
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 2/12
--- Page 3 ---
Thes e w ere m ostly downstream of the number of comments that needed to be
processed. Bu t some had to do w ith the range of attachments I w as processing, m y
comfort in terms of how solid the engineering needed to be (like, d eciding that w e d
need to process m ost of the attachments, b ut not the very m ost diffi cult ones), a nd a
all the w ays that re ality intruded on m y neat architectural plans.
M y previous pipelines had a luxury I didn't appreciate: they w ere small, o n the orde
hundreds or a few thousand ro ws.
Th is had two benefits.
First, w hen I changed something, I needed functionality to test on a small subset to
if it w as w orking, bu t then I could just re -run m y w hole pipeline.
But w hen you’re trying to periodically update a w ebsite as new data comes in, a nd y
have tens of thousands of comments? Th at's different.
Suddenly I needed the ability to pick up w hen new data w as added, instead of starti
at the beginning. I also needed a lot m ore re try logic, logging/m onitoring, a nd
robu stness as far as ru nning w hen I w asn't sitting at m y computer.
And  the res uming req uirement cascaded through everything. F or instance, I built a
manual rel abeling tool to check the LLM 's categorizations and change them w hen
they w ere incorrect. The f rontend for this w asn't complicated—i t w as an interface t
show comments and let m e change labels. B ut tracking w hat changed w hile
maintaining the ability to res ume, a s w ell as the ability to run the pipeline from the
start w hile keeping m y corrections? I eventually abandoned it because the complexi
wasn't w orth it.
But also, the AP I I w as using to pull the comments from regulations.gov w as not se
up to pull this m uch data out of it, so I had to switch to the bu lk data download. ThScale Changes Everything7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 3/12
--- Page 4 ---
meant m y pipeline couldn't truly be autonomous—I  had to start by clicking through
and solving the captcha to ask for the updated data set, a nd then download it from t
webs ite and put it in m y rep o folder.
Using the bulk data download meant I could save usage of the API for just
downloading attachments
And  finally, in previous projects, b ecause there w as so little data, I could use a
lightweight JavaScript front end w hich did all of the w ork in the browser, a s oppose
to needing a back end to filter the data. B ut because of the size of the data set this
time, m y lightweight, m odular front end w as no longer going to w ork because it w o
overwhelm the bro wser: I needed an actual database and front end engineering.
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 4/12
--- Page 5 ---
This is my lightweight, modular front end
Luckily, w ith LLM s, I w as able to easily and painlessly spin up a front end m yself w
no fuss.
W ait, I got that w rong. W hat I actually m eant to say w as: I started w orking w ith
M ichael Boyce, w ho engineered the front end so I could focus on the data pipeline.
W orking w ith an engineer m eant I needed different infrastructure blocks—b ut it al
meant w e could bu ild things I couldn't have done alone.
First, a no te on the importance of schemas. W hen I w as doing the front end m yself 
my very m odular w ay, it didn't m atter m uch if a field name changed or I added in a
new one. I had a config file and I could just update it w ith a new field name. N o nee
to touch the .css, .ht ml, o r javascript. S imple.
But w hen someone else is bu ilding the frontend, that schema—t he exact structure a
names of your data fields—b ecomes a contract. Th e frontend expects specific fields
Needing More Infrastructure7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 5/12
--- Page 6 ---
specific formats. C hange the schema, b reak the frontend.
For this project, I added a data validation step to m y pipeline to m ake sure I w as
outputting the fields I said I w ould in the formats I said I w ould.
No w, in m y new projects, I define database schemas up front, I’m cautious about
changing them, and  I upload the data directly to PostgreSQL m yself.
It’s a database schema!
Custom development also opened up possibilities I hadn't considered. M y previous
approach had some summary statistics, w hich the config file also let you tweak
depending on w hat stats and fields you w anted to show. Th e m ain event w as the tab
which you could filter and search.
But now, w ith custom development, w e could add a line chart showing comments o
time. W e could add embeddings (n umerical representations that let you find similar
comments) and  clustering to group similar ones together.
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 6/12
--- Page 7 ---
Comments over time by position
Th is also m eant that even though I w as no longer handling the front end, there w er
more Lego bl ocks I needed to bu ild to produce the necessary fields for it.
Here's  m y favorite example of bringing the w rong Lego blocks to the project.
I'm  comfortable w ith local text processing of PDFs or W ord documents. S o that's
where I started.
But then I re alized there w as a whole w eird range of documents people attached to
their comments. P DFs that w ere dark and scanned. JPGs of m emes. (R eally!)
The Seven-Cent Epiphany7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 7/12
--- Page 8 ---
This is one of the attachments I processed via Gemini. I’m not saying you
definitely shouldn’t upload these kinds of things in response to proposed
regulations, but maybe think about the data scientist trying to process them for a
moment before you do it
I bu ilt the basic local text extraction w ith PyPDF2 for PDFs and python-docx for W
documents. W hen that w as leaving m e w ith some unprocessed documents, I started
doing local O CRing w ith tesseract to extract text from the images. B ut that w asn't
working great, and  also it w as diffi cult to tell W HEN i t w asn't w orking, b ecause it
would extract text, bu t that text w as sometimes gibberish.
I w as rel uctant to add G emini—G oogle's m ultimodal AI  that can read text from
images—t o the process because I w as w orried it w ould add another expense to this
project, o n top of processing comments w ith an O penAI  m odel.
Then I  just tried it.
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 8/12
--- Page 9 ---
It cost m e seven cents to ru n the entire set of attachments that initially failed to
process through PyPDF2 o r python-docx through G emini.
I deleted the local O CR code.
The bi g benefit w as this let m e drop m y janky functionality for figuring out w hethe
OCR'd text w as actual, re al English or crazy w rong-O CR'd text. W hich w as a
capability I re ally did not w ant to have.
But for a different project, w ith a different set of constraints—f or instance, n eeding
process everything locally—I  w ould have had to figure out attachment processing
without just going to G emini. (An d there are various m odels that are bigger and m o
capable than w hat I tried locally this time around.)
I did w ind up reu sing some of m y code from previous projects.
M y approach to LLM  calls from previous repos w as also w hat I needed here: to pass
class w ith a particular structure—a  template for w hat kind of response I w anted bac
For instance, to get back a list of themes w here it w ould only choose from the ones 
gave it and not m ake up any m ore, o r an "agree/disagree/neutral" for the comment
overall categorization.Reusability Where I Didn't Need It7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 9/12
--- Page 10 ---
By passing these objects to the model, we can make sure the model only chooses
from the list of texts we’re giving it
But this w as a tiny part of the overall system. An d even for the LLM  calls, there w er
bespoke elements of the pipeline.
For instance, a l ot of the comments w ere repeats of each other, so I didn't w ant to s
them to the LLM  individually. An d also, some of them w ere extremely long. Instead
sending each comment in full, I both truncated each comment to its first 1,000
characters and only sent that unique text once.
Th at req uired bu ilding infrastructure to track w hich truncated text m apped to w hic
comments, and  then m erge it back into the original dataset—b ecause w e didn’t w an
to lose the full text.
Th is cut m y LLM  costs and time to run, b ut w hat other project w ould need exactly t
optimization?
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 10/12
--- Page 11 ---
You can bu ild reu sable solutions, b ut they only w ork w ithin their design constraint
Step outside those constraints and you're  back to custom code.
If your pipelines are re ally standardized, y ou only need a few Lego blocks. S ame sca
same format, same output? G reat, re use everything.
But each new surprise—35,000  comments, m emes as attachments, the need to trunc
and group by —m eans m ore bl ocks. An d if you're building tools for other people's
problems that you haven't even seen yet? You need w ay m ore blocks than you think
M y lightweight frontend w orked great until it didn't. M y local text processing hand
everything until someone attached a blurry JPEG. S even cents to G emini later, I
deleted a w hole chunk of code I w as thrilled to delete and hope to never need again
The p attern stays the same—g et text, p rocess it, d o something w ith results. B ut the
blocks keep m ultiplying.
M y next pipeline? No  LLM , no  clustering (y et)—j ust AP Is and w eb scraping and the
mapping the data to standard fields.
Same structure, new  Lego bl ocks. A gain.
Thanks for reading The Present of Coding!
Subscribe for free to receive new posts and
support my work.
1 Like
Discussion about this postThe Next Pipeline
7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 11/12
--- Page 12 ---
Write a comment...
© 2025Abigail Haddad ∙ Privacy ∙ Terms ∙ Collection notice
Substackis the home for great cultureCommentsRestacks7/14/25, 1:00 PM (1) My Text Pipelines Meet The Real World - by Abigail Haddad
https://presentofcoding.substack.com/p/my-text-pipelines-meet-the-real-world 12/12